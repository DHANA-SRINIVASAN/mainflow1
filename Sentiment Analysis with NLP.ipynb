{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8ff1d5e-9b84-4f84-ad8c-a0fb1d0ddd20",
   "metadata": {},
   "source": [
    "Task 2: Sentiment Analysis with Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efe00917-8924-41fe-b090-53add537a8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "310d5309-4c11-409b-822d-df6f8575cde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dhana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dhana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dhana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Processed Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Extremely durable and reliable.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>extremely durable reliable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extremely durable and reliable.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>extremely durable reliable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great customer support, resolved my issue quic...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>great customer support resolved issue quickly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The worst product I've ever used.</td>\n",
       "      <td>Negative</td>\n",
       "      <td>worst product ive ever used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Battery life is awful, wouldn't recommend.</td>\n",
       "      <td>Negative</td>\n",
       "      <td>battery life awful wouldnt recommend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review Text Sentiment  \\\n",
       "0                    Extremely durable and reliable.  Positive   \n",
       "1                    Extremely durable and reliable.  Positive   \n",
       "2  Great customer support, resolved my issue quic...  Positive   \n",
       "3                  The worst product I've ever used.  Negative   \n",
       "4         Battery life is awful, wouldn't recommend.  Negative   \n",
       "\n",
       "                                Processed Review  \n",
       "0                     extremely durable reliable  \n",
       "1                     extremely durable reliable  \n",
       "2  great customer support resolved issue quickly  \n",
       "3                    worst product ive ever used  \n",
       "4           battery life awful wouldnt recommend  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\dhana\\Downloads\\large_synthetic_reviews.csv\")\n",
    "\n",
    "# Initialize stopwords, lemmatizer, and punctuation list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    # Join tokens back into a string\n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "# Apply preprocessing to the \"Review Text\" column\n",
    "df[\"Processed Review\"] = df[\"Review Text\"].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows of the processed dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42429ddf-724f-440c-9dad-8b05107e615c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['review' 'sentiment' 'text']\n",
      "TF-IDF Matrix:\n",
      " [[0.70710678 0.         0.70710678]\n",
      " [0.         1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "doc = pd.read_csv(r\"C:\\Users\\dhana\\Downloads\\large_synthetic_reviews.csv\")\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(doc)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Feature Names:\", feature_names)\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9aa3aeeb-9bfa-451f-b0cf-2f09784cb522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00        20\n",
      "    Positive       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer_large = TfidfVectorizer(max_features=5000)\n",
    "X_large_tfidf = vectorizer_large.fit_transform(doc[\"Review Text\"])\n",
    "y_large = doc[\"Sentiment\"]\n",
    "X_train_large, X_test_large, y_train_large, y_test_large = train_test_split(\n",
    "    X_large_tfidf, y_large, test_size=0.2, random_state=42, stratify=y_large)\n",
    "model_large = LogisticRegression()\n",
    "model_large.fit(X_train_large, y_train_large)\n",
    "y_pred_large = model_large.predict(X_test_large)\n",
    "accuracy_large = accuracy_score(y_test_large, y_pred_large)\n",
    "classification_rep_large = classification_report(y_test_large, y_pred_large)\n",
    "print(f\"Accuracy: {accuracy_large * 100:.2f}%\")\n",
    "print(\"Classification Report:\\n\", classification_rep_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95193a2c-830a-459e-837a-4a8fe2d4cd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1-Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate individual metrics\n",
    "precision = precision_score(y_test_large, y_pred_large, pos_label=\"Positive\")\n",
    "recall = recall_score(y_test_large, y_pred_large, pos_label=\"Positive\")\n",
    "f1 = f1_score(y_test_large, y_pred_large, pos_label=\"Positive\")\n",
    "\n",
    "# Print Evaluation Metrics\n",
    "print(f\"Accuracy: {accuracy_large * 100:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b157f001-ac6d-4837-a6a4-4154deb9e021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
